{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Input_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PulWZ1EYdoZr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "df0bd6f1-e1a1-41ff-bf77-e2c1668cd41a"
      },
      "source": [
        "!pip3 install Distance\n",
        "!pip3 install fuzzywuzzy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from subprocess import check_output #Unknown\n",
        "%matplotlib inline \n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls \n",
        "import os\n",
        "import datetime as dt\n",
        "import gc #Garbage Collection\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import distance \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words('english')\n",
        "from sklearn.manifold import TSNE\n",
        "from PIL import Image \n",
        "from fuzzywuzzy import fuzz\n",
        "import time \n",
        "import warnings \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sys #Works with parameter\n",
        "from tqdm import tqdm\n",
        "import spacy #Works with different languages\n",
        "import sqlite3\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.decomposition import TruncatedSVD #Use for dimension reduction\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.classification import accuracy_score, log_loss\n",
        "from collections import Counter #Uses for value count\n",
        "from scipy.sparse import hstack #Uses to join multiple matrics\n",
        "from sklearn.multiclass import OneVsRestClassifier #predict class\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Distance\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Distance\n",
            "  Building wheel for Distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Distance: filename=Distance-0.1.3-cp36-none-any.whl size=16261 sha256=9d2dc2e4d9d261874a81ecd7c2528e67d7845112dfc70b2026154d29ecceb08e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "Successfully built Distance\n",
            "Installing collected packages: Distance\n",
            "Successfully installed Distance-0.1.3\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCB0H427eX56"
      },
      "source": [
        "SAFE_DIV = 0.0001 # decimal place\n",
        "def fillna(df):\n",
        "    df = df.fillna('')\n",
        "    nan_rows = df[df.isnull().any(1)]\n",
        "\n",
        "\n",
        "\n",
        "def feature_without_pp():\n",
        "        df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
        "        df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
        "        df['q1len'] = df['question1'].str.len() \n",
        "        df['q2len'] = df['question2'].str.len()\n",
        "        df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "        df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "\n",
        "        def normalized_word_Common(row):\n",
        "            w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "            w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "            return 1.0 * len(w1 & w2)\n",
        "        df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
        "        def normalized_word_Total(row):\n",
        "            w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "            w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "            return 1.0 * (len(w1) + len(w2))\n",
        "        df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
        "\n",
        "        def normalized_word_share(row):\n",
        "            w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "            w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "            return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
        "        df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
        "\n",
        "        df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
        "        df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
        "\n",
        "        df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n",
        "        return df\n",
        "\n",
        "\n",
        "def feature_without_pp():\n",
        "        df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
        "        df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
        "        df['q1len'] = df['question1'].str.len() \n",
        "        df['q2len'] = df['question2'].str.len()\n",
        "        df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "        df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "\n",
        "        def normalized_word_Common(row):\n",
        "            w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "            w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "            return 1.0 * len(w1 & w2)\n",
        "        df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
        "        def normalized_word_Total(row):\n",
        "            w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "            w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "            return 1.0 * (len(w1) + len(w2))\n",
        "        df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
        "\n",
        "        def normalized_word_share(row):\n",
        "            w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "            w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "            return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
        "        df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
        "\n",
        "        df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
        "        df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
        "\n",
        "        df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "#Function for preprocessing data\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "    \n",
        "    STOP_WORDS = stopwords.words(\"english\")\n",
        "    x = str(x).lower()\n",
        "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
        "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
        "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
        "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
        "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
        "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
        "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
        "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
        "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
        "    porter = PorterStemmer() # Convert all similar words to theri ideal word \n",
        "    pattern = re.compile('\\W') #Find white space\n",
        "    if type(x) == type(''):\n",
        "        x = re.sub(pattern, ' ', x)\n",
        "    \n",
        "    if type(x) == type(''):\n",
        "        x = porter.stem(x)\n",
        "        example1 = BeautifulSoup(x) # Remove all html tags\n",
        "        x = example1.get_text()\n",
        "    return x\n",
        "\n",
        "\n",
        "#Funtion to create features with preprocessing data\n",
        "\n",
        "def get_token_features(q1, q2):\n",
        "    token_features = [0.0]*10\n",
        "    \n",
        "    q1_tokens = q1.split()\n",
        "    q2_tokens = q2.split()\n",
        "\n",
        "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
        "        return token_features\n",
        "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
        "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
        "\n",
        "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
        "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
        "    \n",
        "    common_word_count = len(q1_words.intersection(q2_words))\n",
        "    \n",
        "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
        "    \n",
        "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
        "    \n",
        "    \n",
        "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
        "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
        "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
        "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
        "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
        "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
        "    \n",
        "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
        "    \n",
        "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
        "    \n",
        "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
        "    \n",
        "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
        "    \n",
        "    return token_features \n",
        "\n",
        "\n",
        "#Funtion to create features with preprocessing data and to preprocess data...\n",
        "\n",
        "def get_longest_substr_ratio(a, b):\n",
        "    strs = list(distance.lcsubstrings(a, b))\n",
        "    if len(strs) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
        "\n",
        "def extract_features(df):\n",
        "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
        "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
        "\n",
        "    \n",
        "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    \n",
        "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
        "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
        "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
        "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
        "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
        "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
        "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
        "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
        "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
        "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
        "   \n",
        "\n",
        "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_feature(alamin):\n",
        "    df = extract_features(alamin)\n",
        "    df.to_csv(\"nlp_features_train.csv\", index=False)\n",
        "\n",
        "\n",
        "def tfidfvec(df):\n",
        "\n",
        "    df = df\n",
        "    df['question1'] = df['question1'].apply(lambda x: str(x))\n",
        "    df['question2'] = df['question2'].apply(lambda x: str(x))\n",
        "    questions = list(df['question1']) + list(df['question2'])\n",
        "    tfidf = TfidfVectorizer(lowercase=False, )\n",
        "    tfidf.fit_transform(questions)\n",
        "    word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
        "\n",
        "\n",
        "    import en_core_web_sm\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    vecs1 = []\n",
        "\n",
        "    for qu1 in tqdm(list(df['question1'])):\n",
        "        doc1 = nlp(qu1) \n",
        "        mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n",
        "        for word1 in doc1:\n",
        "            vec1 = word1.vector\n",
        "            try:\n",
        "                idf = word2tfidf[str(word1)]\n",
        "            except:\n",
        "                idf = 0\n",
        "            mean_vec1 += vec1 * idf\n",
        "        mean_vec1 = mean_vec1.mean(axis=0)\n",
        "        vecs1.append(mean_vec1)\n",
        "    df['q1_feats_m'] = list(vecs1)\n",
        "\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    vecs2 = []\n",
        "    for qu2 in tqdm(list(df['question2'])):\n",
        "        doc2 = nlp(qu2) \n",
        "        mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n",
        "        for word2 in doc2:\n",
        "            vec2 = word2.vector\n",
        "            try:\n",
        "                idf = word2tfidf[str(word2)]\n",
        "            except:\n",
        "\n",
        "                idf = 0\n",
        "            mean_vec2 += vec2 * idf\n",
        "        mean_vec2 = mean_vec2.mean(axis=0)\n",
        "        vecs2.append(mean_vec2)\n",
        "    df['q2_feats_m'] = list(vecs2)\n",
        "\n",
        "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
        "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
        "    df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
        "    df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "    df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "    df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
        "    df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)\n",
        "\n",
        "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
        "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
        "    df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
        "    df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "    df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "    df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
        "    df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)\n",
        "    df3_q1['id']=df1['id']\n",
        "    df3_q2['id']=df1['id']\n",
        "    df1  = df1.merge(df2, on='id',how='left')\n",
        "    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n",
        "    result  = df1.merge(df2, on='id',how='left')\n",
        "    result.to_csv('final_features.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4dz5FcYSoGm"
      },
      "source": [
        "# Logistic_Regression_Model\n",
        "\n",
        "def LR (test_file):\n",
        "    test_file = test_file.fillna(0)\n",
        "    print(\"\\n\")\n",
        "    print(\"Logistic Regression Model\\n\")\n",
        "\n",
        "    from sklearn.externals import joblib\n",
        "    model = joblib.load('/content/drive/My Drive/Machine Learning/Real World Problem-1/quora_logistic_regression_model.pkl')\n",
        "    print(model.predict_proba(test_file))\n",
        "    print(model.predict(test_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK621WK7vsbg"
      },
      "source": [
        "# XGBoost Model\n",
        "\n",
        "def XB(test_file):\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"XGBoos model prediction....\\n\")\n",
        "    from sklearn.externals import joblib\n",
        "    import xgboost as xgb\n",
        "    model = joblib.load('/content/drive/My Drive/Machine Learning/Real World Problem-1/quora_xgboost_model.pkl')\n",
        "    test_data = xgb.DMatrix(test_file)\n",
        "    \n",
        "    predicted_value = model.predict(test_data)\n",
        "    print(predicted_value)\n",
        "    predicted_result =np.array(predicted_value>0.5,dtype=int)\n",
        "    print(predicted_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7o5Ontl28wX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "afeac7a8-b80b-484f-dfc5-b1ec5264bd5d"
      },
      "source": [
        "#Input test file\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Machine Learning/Real World Problem-1/dataFrame.csv', index_col = 0)\n",
        "print(df)\n",
        "df['question1'] = input(\"Enter your question-1: \")\n",
        "df['question2'] = input(\"Enter your question-2: \")\n",
        "df_1 = df.copy()\n",
        "df_2 = df.copy()\n",
        "\n",
        "feature_without_pp()\n",
        "create_feature(df_1)\n",
        "tfidfvec(df_2)\n",
        "\n",
        "\n",
        "test_file = pd.read_csv('/content/final_features.csv')\n",
        "test_file.drop(['Unnamed: 0', 'id','is_duplicate'], axis=1, inplace=True)\n",
        "cols = list(test_file.columns)\n",
        "for i in cols:\n",
        "    test_file[i] = test_file[i].apply(pd.to_numeric)\n",
        "\n",
        "XB(test_file)\n",
        "LR(test_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  qid1  qid2 question1 question2  is_duplicate\n",
            "0   0     1     2  I am man  I am man             0\n",
            "Enter your question-1: what will i do to be a engineer?\n",
            "Enter your question-2: what to do to be a engineer?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 65.92it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 67.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "XGBoos model prediction....\n",
            "\n",
            "[0.5729551]\n",
            "[1]\n",
            "\n",
            "\n",
            "Logistic Regression Model\n",
            "\n",
            "[[0.44906571 0.55093429]]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}